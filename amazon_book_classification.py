# -*- coding: utf-8 -*-
"""Amazon Book Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16chU-4eZzDxG_uNkGpoeyBc2J5PWcle0
"""

#importing the necessary libraries and dependencies
import pandas as pd
import numpy as np
import seaborn as sn
import matplotlib.pyplot as plt
import re
import nltk
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB,MultinomialNB
from sklearn.svm import SVC
from sklearn import metrics

#reading book summary csv file
amz_books = pd.read_csv('BooksDataSet.csv')
amz_books.head()

## removing the unnamed column
amz_books = amz_books[['book_id', 'book_name', 'genre', 'summary']]
amz_books.head()

#checking the structure of the dataframe including null values
amz_books.info()

# Creating a count plot to visualize the distribution of genres in the 'genre' column
sn.countplot(x = amz_books['genre'], palette = 'plasma' )
plt.xticks(rotation = 'vertical')

# Checking the summary of sample data to understand the content and determine what to include in the cleaning function
amz_books['summary'].iloc[1]

def text_cleaning(text):
  text = re.sub("'\''", "", text)  # Remove backslashes followed by single quotes
  text = re.sub("[^a-zA-Z]", " ", text)  # Replace non-alphabetic characters with spaces
  text = ' '.join(text.split())  # Remove extra whitespace
  text = text.lower()  # Convert text to lowercase
  return text

# Apply the text_cleaning function to the 'summary' column of amz_books
amz_books['summary'] = amz_books['summary'].apply(lambda x: text_cleaning(x))

# Print the cleaned summary of the second row
print(amz_books['summary'].iloc[1])

#plotting the most frequent words 
#these are mostly stopwords and they do not affect the meaning of the sentence
def frequentWords(text, num_words):

  # Combine all characters in the text into a single string
  words = ' '.join([char for char in text])

  # Split the string into individual words
  words = words.split()

  # Compute the frequency distribution of words
  distribution = nltk.FreqDist(words)

  # Create a DataFrame to store the word counts
  df_words = pd.DataFrame({'word': list(distribution.keys()), 'count':list(distribution.values())})

  # Select the top 'num_words' words with the highest count
  df = df_words.nlargest(columns ="count", n = num_words)

  # Create a bar plot to visualize the word counts
  plt.figure(figsize=(7,5))
  ax = sn.barplot(data= df, x = 'count', y = 'word')
  ax.set(ylabel = 'word')
  plt.show()

  return df_words

df_words = frequentWords(amz_books['summary'], 20)
df_words.sort_values('count',ascending = False).head(10).style.background_gradient(cmap = 'plasma')

##downloading the stops words of nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))

# defining the stopwords removal function
def remove_stopwords(text):
  removed_words = [word for word in text.split() if word not in stop_words]
  return ' '.join(removed_words)

amz_books['summary'] = amz_books['summary'].apply(lambda x:remove_stopwords(x))
amz_books['summary'].iloc[1]

#lemmatizing
nltk.download('wordnet')

from nltk.tag.brill import Word
from nltk.stem import WordNetLemmatizer
word_lemma = WordNetLemmatizer()

def lemmatizing(sentence):
  stemSentence = ""
  for word in sentence.split():
    stemmed = word_lemma.lemmatize(word)
    stemSentence += stemmed
    stemSentence += " "
  stemSentence = stemSentence.strip()
  return stemSentence

amz_books['summary'] = amz_books['summary'].apply(lambda x:lemmatizing(x))

#stemming
from nltk.stem import PorterStemmer
stemmer = PorterStemmer()

def stemming(sentence):
  stemmed_sentence = ""
  for word in sentence.split():
    stem = stemmer.stem(word)
    stemmed_sentence += stem
    stemmed_sentence += " "
  
  stemmed_sentence = stemmed_sentence.strip()
  return stemmed_sentence

amz_books['summary'] = amz_books['summary'].apply(lambda text: stemming(text))
amz_books['summary'].iloc[1]

df_words = frequentWords(amz_books['summary'], 20)
df_words.sort_values('count', ascending = False).head(10).style.background_gradient(cmap = 'plasma')

"""# Encoding genres for unique reference"""

#At this stage we are getting all the unique values in the genre column for encoding
genre_list = list(amz_books['genre'].unique())
encoded = [i for i in range(len(genre_list))]
mapper = dict(zip(genre_list, encoded))
print(mapper)

amz_books['genre'] = amz_books['genre'].map(mapper)
amz_books['genre'].unique()

"""# Model Building
Converting text in to the numerical representation using vectorizer
"""

vectorized_text = CountVectorizer(max_df = 0.90, min_df = 2,
                                  max_features = 1000, stop_words = 'english')
vectorized_word = vectorized_text.fit_transform(amz_books['summary'])
vectorized_word

test = amz_books['genre']
X_train, X_test, y_train, y_test = train_test_split(vectorized_word, test, test_size = 0.2)
X_train.shape, X_test.shape

svc = SVC()
svc.fit(X_train, y_train)
predictions_svc = svc.predict(X_test)
print(metrics.accuracy_score(y_test,predictions_svc))

###Multinomial Distribution
multiDist = MultinomialNB()
multiDist.fit(X_train, y_train)
multiDist_pred = multiDist.predict(X_test)
print(metrics.accuracy_score(y_test, multiDist_pred))

random_f = RandomForestClassifier()
random_f.fit(X_train, y_train)
print(metrics.accuracy_score(y_test,random_f.predict(X_test)))

"""With Count vectorizing average results we will try the TDIF vectorizer"""

##Labeling the genre
from sklearn.preprocessing import LabelEncoder
label_encode = LabelEncoder()
fitting = label_encode.fit_transform(amz_books['genre'])

X_train, X_test, y_train, y_test = train_test_split(amz_books['summary'], fitting, test_size=0.2, random_state=557)

X_train.shape, X_test.shape

tfid_vector = TfidfVectorizer(max_df=0.8, max_features = 10000)
tfid_xtrain = tfid_vector.fit_transform(X_train.values.astype('U'))
tfid_xtest = tfid_vector.transform(X_test.values.astype('U'))

svc.fit(tfid_xtrain, y_train)
predicted_svc = svc.predict(tfid_xtest)
print(metrics.accuracy_score(y_test,predicted_svc))

multiDist.fit(tfid_xtrain, y_train)
tf_multiDist = multiDist.predict(tfid_xtest)
print(metrics.accuracy_score(y_test, tf_multiDist))

def test(text, model):
  text = text_cleaning(text)
  text = remove_stopwords(text)
  text = lemmatizing(text)
  text = stemming(text)

  vectorized_text = tfid_vector.transform([text])
  predicted = model.predict(vectorized_text)
  return predicted

amz = amz_books['summary'].apply(lambda text:test(text,multiDist))
amz

amz[0][0]

#assigns predicted class labels to the predicted_class 
#list based on the values in the amz list and a mapper dictionary.
predicted_class = []
for i in range(len(amz)):
  index_val = amz[i][0]
  predicted_class.append(list(mapper.keys())[list(mapper.values()).index(index_val)])

updated_mapper = dict([(value, key) for key, value in mapper.items()])
updated_mapper

amz_books['Actual Genre'] = amz_books['genre'].map(updated_mapper)
amz_books['Predicted Genre'] = np.array(predicted_class)
amz_books

